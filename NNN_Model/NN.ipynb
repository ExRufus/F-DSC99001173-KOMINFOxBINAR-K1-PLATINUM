{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d80d6e81-2490-4e19-a943-01ebd7dbcaf7",
   "metadata": {},
   "source": [
    "## Call the Abbys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0232d22-a4ba-417a-98db-92e4d648dbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import sqlite3\n",
    "import pickle  # To save the model\n",
    "\n",
    "# Scikit-learn modules for machine learning pipeline and model tuning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Scikit-learn modules for feature extraction (text vectorization)\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Machine learning algorithms\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Scikit-learn metrics for model evaluation\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90c3a6f8-05bc-4017-8cdc-f933b8c1d4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teks</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>warung dimiliki pengusaha pabrik tahu puluhan ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mohon ulama lurus mmbri hujjah partai diwlh su...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lokasi strategis jalan sumatra bandung nyaman ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>betapa bahagia unboxing paket barang bagus men...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aduh mahasiswa jangan sombong dong kasih kartu...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Teks     Label\n",
       "0  warung dimiliki pengusaha pabrik tahu puluhan ...  positive\n",
       "1  mohon ulama lurus mmbri hujjah partai diwlh su...   neutral\n",
       "2  lokasi strategis jalan sumatra bandung nyaman ...  positive\n",
       "3  betapa bahagia unboxing paket barang bagus men...  positive\n",
       "4  aduh mahasiswa jangan sombong dong kasih kartu...  negative"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Establish a connection to the SQLite database\n",
    "db_path = r'E:\\Sentimen\\sentimen\\data.db'\n",
    "connection = sqlite3.connect(db_path, check_same_thread=False)\n",
    "\n",
    "# Define SQL query to retrieve data from 'a_train' table\n",
    "sql_query = 'SELECT * FROM a_trainb'\n",
    "\n",
    "# Execute the query and load data into a DataFrame\n",
    "a_train_data = pd.read_sql(sql_query, connection)\n",
    "\n",
    "# Display the first few rows of the retrieved data\n",
    "a_train_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec4217d1-3b94-4dcd-b78a-001f4e865821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "positive    6416\n",
      "negative    3436\n",
      "neutral     1148\n",
      "Name: count, dtype: int64\n",
      "Total count: 11000\n"
     ]
    }
   ],
   "source": [
    "# Menghitung jumlah kemunculan masing-masing nilai di kolom 'label'\n",
    "label_counts = a_train_data['Label'].value_counts()\n",
    "\n",
    "# Menampilkan nilai frekuensi masing-masing kategori\n",
    "print(label_counts)\n",
    "\n",
    "# Menghitung total jumlah data\n",
    "total_count = label_counts.sum()\n",
    "\n",
    "# Menampilkan total jumlah data\n",
    "print(f\"Total count: {total_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833a0dc2-7021-42fd-a4b4-c8f4754c056e",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "672fe079-d81a-4d49-8abf-e4a0c1870046",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed = a_train_data.Teks.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11787b81-8f9a-46d9-98c8-72ed9f708b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f884376e-cbe5-4497-a385-dcef8f21f759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Extraction selesai\n"
     ]
    }
   ],
   "source": [
    "# Untuk melakukan Feature Extraction, kita menggunakan library \"Sklearn atau scikit-learn\".\n",
    "# Sklearn adalah library untuk melakukan task-task Machine Learning.\n",
    "# \"CountVectorizer\" merupakan salah satu modul untuk melakukan \"BoW\"\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Kita proses Feature Extraction\n",
    "count_vect = CountVectorizer()\n",
    "count_vect.fit(data_preprocessed)\n",
    "\n",
    "X = count_vect.transform(data_preprocessed)\n",
    "print (\"Feature Extraction selesai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85caad96-122b-48f4-879e-ff9fcc0ac5ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 167026 stored elements and shape (11000, 15320)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7339385-909a-40cf-99bd-951e2e6f6b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(count_vect, open(\"feature.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b93c21a0-6bd6-4230-963b-8ba5f8b5e7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "classes = a_train_data.Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbe66c4e-3b64-442a-ae69-62213c326b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        positive\n",
       "1         neutral\n",
       "2        positive\n",
       "3        positive\n",
       "4        negative\n",
       "           ...   \n",
       "10995    positive\n",
       "10996    positive\n",
       "10997     neutral\n",
       "10998    negative\n",
       "10999    positive\n",
       "Name: Label, Length: 11000, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ef87a8-81aa-416e-8657-7fdb7973f246",
   "metadata": {},
   "source": [
    "melakukan encoding label kategori ke dalam format one-hot encoding agar dapat digunakan dalam model Machine Learning.\n",
    "Data teks dan label yang telah di-encode dipisahkan menjadi fitur (x) dan target (y)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2843e184-9998-43c4-8b6f-25a96e177ddf",
   "metadata": {},
   "source": [
    "Melakukan one-hot encoding pada label kategori penting untuk model machine learning karena model ini umumnya tidak bisa langsung memproses data kategorikal. Encoding ini mengubah label menjadi format biner yang dapat dipahami secara numerik, menghindari misinterpretasi urutan, dan menghilangkan bias dalam model. Dengan membagi data teks dan label yang telah di-encode menjadi fitur (X) dan target (Y), kita memastikan bahwa model dapat melatih dan memprediksi dengan benar menggunakan fungsi aktivasi dan loss function yang sesuai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8e003f-c0d6-4041-80a1-821928a3658a",
   "metadata": {},
   "source": [
    "## Train Test Split x train, x test, y train, y test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d990cc16-c5cb-46ed-8240-16830aeb6c24",
   "metadata": {},
   "source": [
    "membagi data menjadi dua subset: data pelatihan (X_train, y_train) dan data pengujian (X_test, y_test). Dengan test_size=0.2, 20% dari data digunakan untuk pengujian dan 80% untuk pelatihan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "690562f7-bfad-4c14-a66d-d393c1760c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, classes, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a42967-8999-4bc9-9ce3-fedaa3b588b1",
   "metadata": {},
   "source": [
    "Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac1beebb-ea23-4c71-ba74-7552f1577b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training selesai\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model = MLPClassifier() \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print (\"Training selesai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b476a7f5-4acc-4611-a456-4f3d7dc56bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open(\"model.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3393f46a-d6c8-4b79-bcdd-0b94616ebecd",
   "metadata": {},
   "source": [
    "model tampaknya lebih efektif dalam mengklasifikasikan kelas positive dan negative, sementara performanya sedikit kurang untuk kelas neutral. Akurasi makro rata-rata adalah 0.77, menunjukkan performa yang konsisten di berbagai kelas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa5c0b6d-98d0-49a6-af52-2c3dac9bc49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Ok\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.79      0.78       675\n",
      "     neutral       0.79      0.67      0.73       243\n",
      "    positive       0.88      0.89      0.89      1282\n",
      "\n",
      "    accuracy                           0.84      2200\n",
      "   macro avg       0.81      0.79      0.80      2200\n",
      "weighted avg       0.84      0.84      0.84      2200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test = model.predict(X_test)\n",
    "\n",
    "print (\"Testing Ok\")\n",
    "\n",
    "print(classification_report(y_test, test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e1abbb-3544-4fb1-8da5-88f91066ac7f",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9add34b4-b1b8-4b7a-bdaa-31bb7e2e6ebb",
   "metadata": {},
   "source": [
    "cross-validation dengan KFold untuk mengevaluasi model. Data dibagi menjadi 5 lipatan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62f0e44a-029c-4bc0-a129-86be43db0d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterasi ke- 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.78      0.77       680\n",
      "     neutral       0.77      0.60      0.67       239\n",
      "    positive       0.87      0.89      0.88      1281\n",
      "\n",
      "    accuracy                           0.82      2200\n",
      "   macro avg       0.80      0.76      0.77      2200\n",
      "weighted avg       0.82      0.82      0.82      2200\n",
      "\n",
      "====================\n",
      "Iterasi ke- 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.75      0.76       706\n",
      "     neutral       0.66      0.65      0.66       220\n",
      "    positive       0.87      0.89      0.88      1274\n",
      "\n",
      "    accuracy                           0.82      2200\n",
      "   macro avg       0.77      0.77      0.77      2200\n",
      "weighted avg       0.82      0.82      0.82      2200\n",
      "\n",
      "====================\n",
      "Iterasi ke- 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.76      0.77       682\n",
      "     neutral       0.78      0.67      0.72       215\n",
      "    positive       0.87      0.90      0.89      1303\n",
      "\n",
      "    accuracy                           0.83      2200\n",
      "   macro avg       0.81      0.78      0.79      2200\n",
      "weighted avg       0.83      0.83      0.83      2200\n",
      "\n",
      "====================\n",
      "Iterasi ke- 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.78      0.77       698\n",
      "     neutral       0.74      0.59      0.66       229\n",
      "    positive       0.87      0.89      0.88      1273\n",
      "\n",
      "    accuracy                           0.82      2200\n",
      "   macro avg       0.79      0.75      0.77      2200\n",
      "weighted avg       0.82      0.82      0.82      2200\n",
      "\n",
      "====================\n",
      "Iterasi ke- 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.77      0.76       670\n",
      "     neutral       0.76      0.63      0.69       245\n",
      "    positive       0.87      0.89      0.88      1285\n",
      "\n",
      "    accuracy                           0.83      2200\n",
      "   macro avg       0.80      0.77      0.78      2200\n",
      "weighted avg       0.83      0.83      0.83      2200\n",
      "\n",
      "====================\n",
      "\n",
      "\n",
      "\n",
      "Rata-rata Accuracy:  0.8265454545454546\n"
     ]
    }
   ],
   "source": [
    "# \"Cross Validation\"\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5,random_state=42,shuffle=True)\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "y = classes\n",
    "\n",
    "for iteration, data in enumerate(kf.split(X), start=1):\n",
    "\n",
    "    data_train   = X[data[0]]\n",
    "    target_train = y[data[0]]\n",
    "\n",
    "    data_test    = X[data[1]]\n",
    "    target_test  = y[data[1]]\n",
    "\n",
    "    clf = MLPClassifier()\n",
    "    clf.fit(data_train,target_train)\n",
    "\n",
    "    preds = clf.predict(data_test)\n",
    "\n",
    "    # for the current fold only    \n",
    "    accuracy = accuracy_score(target_test,preds)\n",
    "\n",
    "    print(\"Iterasi ke-\", iteration)\n",
    "    print(classification_report(target_test,preds))\n",
    "    print(\"=\"*20)\n",
    "\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# this is the average accuracy over all folds\n",
    "average_accuracy = np.mean(accuracies)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "print(\"Rata-rata Accuracy: \", average_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc35d7e2-8da8-455e-917d-b2f24dd79da8",
   "metadata": {},
   "source": [
    "Secara keseluruhan, model menunjukkan performa yang baik dalam mengklasifikasikan kelas positive dan negative, tetapi perlu diperhatikan bahwa hasil untuk kelas neutral lebih bervariasi dan cenderung lebih rendah. Rata-rata akurasi yang stabil menunjukkan bahwa model tidak terlalu overfit pada data latih dan generalisasinya baik pada data uji."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c452bb86-96cb-4cf9-9851-c904955801eb",
   "metadata": {},
   "source": [
    "## Predict Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37af07f-0f28-47ef-948c-cfcbcd6ab549",
   "metadata": {},
   "source": [
    "clean_text(text): Membersihkan teks dengan mengubahnya menjadi huruf kecil, menghapus URL, karakter non-alfabet, dan spasi berlebih.\n",
    "\n",
    "standardize_text(text): Mengganti kata-kata \"alay\" dengan bentuk baku menggunakan kamus dari database.\n",
    "\n",
    "remove_stopwords(text): Menghapus kata-kata berhenti dari teks jika diperlukan (fungsi ini saat ini dikomentari).\n",
    "\n",
    "preprocess_input(text): Menggabungkan pembersihan teks dan standarisasi teks, serta menghapus stopwords jika diaktifkan.\n",
    "\n",
    "Feature Extraction dan Prediksi: Mengubah teks yang diproses menjadi fitur yang bisa dipahami oleh model dan menggunakan model untuk memprediksi sentimen teks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28e9b404-45b6-4113-ac48-d77259b8a292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text:\n",
      "Dia mah apa si jarwo, udah jelek item dan bau bawang\n",
      "\n",
      "Text Preprocessing:\n",
      "dia adalah apa sih jarwo sudah jelek hitam dan bau bawang\n",
      "\n",
      "Feature Extraction:\n",
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 5 stored elements and shape (1, 15320)>\n",
      "  Coords\tValues\n",
      "  (0, 949)\t1\n",
      "  (0, 957)\t1\n",
      "  (0, 5210)\t1\n",
      "  (0, 5773)\t1\n",
      "  (0, 12961)\t1\n",
      "\n",
      "Sentiment:\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Menghubungkan ke database SQLite\n",
    "db_path = r'E:\\Sentimen\\sentimen\\data.db'\n",
    "connection = sqlite3.connect(db_path)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Ubah teks menjadi huruf kecil\n",
    "    text = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', '', text)  # Hapus URL dan tautan\n",
    "    text = re.sub(r'pic.twitter.com\\.\\w+', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)  # Hapus karakter yang tidak diinginkan, termasuk angka\n",
    "    text = text.replace('user', '')  # Hapus kata 'user'\n",
    "    text = re.sub(' +', ' ', text)  # Hapus spasi berlebih\n",
    "    text = text.replace('\\n', ' ')  # Hapus karakter \\n (newline)\n",
    "    return text.strip()\n",
    "\n",
    "def standardize_text(text):\n",
    "    alay_df = pd.read_sql_query('SELECT * FROM kamus_alay', connection)\n",
    "    alay_dict = dict(zip(alay_df['alay'], alay_df['fix']))  # Buat kamus dari teks alay ke baku\n",
    "    words = text.split()\n",
    "    \n",
    "    standardized_words = [alay_dict.get(word, word) for word in words]  # Ganti teks alay dengan baku\n",
    "    return ' '.join(standardized_words).strip()\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stopword_query = 'SELECT * FROM stopword'\n",
    "    stopword_df = pd.read_sql_query(stopword_query, connection)\n",
    "    stopwords = set(stopword_df['stop'])\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stopwords]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def preprocess_input(text):\n",
    "    text = clean_text(text)\n",
    "    text = standardize_text(text)\n",
    "    # text = remove_stopwords(text)  # Uncomment jika perlu menghapus stopword\n",
    "    return text\n",
    "\n",
    "# Load model and CountVectorizer from files\n",
    "with open('feature.pkl', 'rb') as f:\n",
    "    count_vect = pickle.load(f)\n",
    "    \n",
    "with open('model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# Original text\n",
    "original_text = 'Dia mah apa si jarwo, udah jelek item dan bau bawang'\n",
    "\n",
    "# Preprocess the input text\n",
    "preprocessed_text = preprocess_input(original_text)\n",
    "\n",
    "# Feature Extraction\n",
    "text_features = count_vect.transform([preprocessed_text])\n",
    "\n",
    "# Display feature extraction\n",
    "print(\"Input Text:\")\n",
    "print(original_text)\n",
    "print(\"\\nText Preprocessing:\")\n",
    "print(preprocessed_text)\n",
    "print(\"\\nFeature Extraction:\")\n",
    "print(text_features)\n",
    "\n",
    "# Predict sentiment\n",
    "result = model.predict(text_features)[0]\n",
    "print(\"\\nSentiment:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba891e3d-9495-4b9f-a4f6-166670e30cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = pickle.load(open('feature.pkl', 'rb'))\n",
    "model_NN=pickle.load(open('model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2c42ac3-71ef-48b5-973f-ebd2379a89c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks Asli: Si Jarwo teh, emg bodo bgt\n",
      "Teks Setelah Cleaning: sih jarwo teh memang bodoh banget\n",
      "Sentimen:\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Menghubungkan ke database SQLite\n",
    "db_path = r'E:\\Sentimen\\sentimen\\data.db'\n",
    "connection = sqlite3.connect(db_path)\n",
    "\n",
    "# Fungsi membersihkan teks\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', '', text)  # Hapus URL dan tautan\n",
    "    text = re.sub(r'pic.twitter.com\\.\\w+', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text.lower())  # Hapus karakter yang tidak diinginkan, termasuk angka\n",
    "    text = text.replace('user', '')  # Hapus kata 'user'\n",
    "    text = re.sub(' +', ' ', text)  # Hapus spasi berlebih\n",
    "    text = text.replace('\\n', ' ')  # Hapus karakter \\n (newline)\n",
    "    return text.strip()\n",
    "\n",
    "# Fungsi standarisasi teks (ganti teks alay dengan baku)\n",
    "def standardize_text(text):\n",
    "    alay_df = pd.read_sql_query('SELECT * FROM kamus_alay', connection)\n",
    "    alay_dict = dict(zip(alay_df['alay'], alay_df['fix']))  # Buat kamus dari teks alay ke baku\n",
    "    words = text.split()\n",
    "    standardized_words = [alay_dict.get(word, word) for word in words]  # Ganti teks alay dengan baku\n",
    "    return ' '.join(standardized_words).strip()\n",
    "\n",
    "# Fungsi hapus stopwords\n",
    "def remove_stopwords(text):\n",
    "    stopword_query = 'SELECT * FROM stopword'\n",
    "    stopword_df = pd.read_sql_query(stopword_query, connection)\n",
    "    stopwords = set(stopword_df['stop'])\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stopwords]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Fungsi untuk melakukan preprocessing teks input dan mengembalikan teks asli dan yang sudah dibersihkan\n",
    "def preprocess_input(text):\n",
    "    original_text = text  # Simpan teks asli\n",
    "    text = clean_text(text)\n",
    "    text = standardize_text(text)\n",
    "    # Jika ingin menghapus stopwords, uncomment ini\n",
    "    # text = remove_stopwords(text)\n",
    "    cleaned_text = text  # Simpan teks yang telah dibersihkan\n",
    "    return original_text, cleaned_text\n",
    "\n",
    "# Mengambil teks yang akan diprediksi\n",
    "original_text = \"Si Jarwo teh, emg bodo bgt\"\n",
    "\n",
    "# Preprocessing teks dan menampilkan hasilnya\n",
    "original_text, cleaned_text = preprocess_input(original_text)\n",
    "\n",
    "# Tampilkan teks asli dan teks hasil preprocessing\n",
    "print(f\"Teks Asli: {original_text}\")\n",
    "print(f\"Teks Setelah Cleaning: {cleaned_text}\")\n",
    "\n",
    "# Feature Extraction (misalnya menggunakan load_model transform untuk vektorisasi)\n",
    "# Misalkan 'load_model' adalah model yang digunakan untuk ekstraksi fitur teks\n",
    "text = load_model.transform([cleaned_text])  # Pastikan sesuai dengan cara ekstraksi fitur yang digunakan\n",
    "\n",
    "# Kita prediksi sentimennya\n",
    "result = model_NN.predict(text)[0]\n",
    "\n",
    "# Tampilkan hasil prediksi\n",
    "print(\"Sentimen:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f949698-7847-4c45-9d77-a7c8d3c1ef3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_sentimen",
   "language": "python",
   "name": "venv_sentimen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
